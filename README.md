# API Documentation Training Pipeline

> **A comprehensive machine learning pipeline for generating and improving API documentation using state-of-the-art language models.**

This repository contains a complete pipeline for automated API documentation generation, paraphrasing, and enhancement using various pre-trained and fine-tuned language models including SmolLM2, Qwen2.5-Coder, T5, and more.

## ğŸ¯ Project Overview

The project consists of three main components:

1. **ğŸ“– Website Content Extraction & Q&A Generation** - Crawl documentation websites and create Q&A training data
2. **ğŸ§  Code Documentation Generation** - Train models to generate documentation from source code
3. **ğŸ”„ Text Paraphrasing & Enhancement** - Improve existing documentation through rewriting and style transformation

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- CUDA-compatible GPU (recommended)
- 8GB+ RAM

### Installation

```bash
# Clone the repository
git clone https://github.com/sarpsl/APIDocumentation.git
cd APIDocumentation

# Install dependencies
pip install -r requirements_pipeline.txt
```

### Quick Test

```bash
# Run a quick test with minimal training
python quick_start_pipeline.py
```

### Full Pipeline

```bash
# Run the complete pipeline
python full_training_pipeline.py
```

## ğŸ“‹ Features & Capabilities

### ğŸŒ Website Content Processing
- **Recursive web crawling** with configurable depth and filtering
- **Content cleaning** and structured extraction
- **Automatic Q&A pair generation** from documentation content
- **Smart filtering** to exclude non-relevant files (PDFs, images, etc.)

### ğŸ’» Code Documentation Generation
- **Multi-model support**: SmolLM2-360M-Instruct, Qwen2.5-Coder-0.5B-Instruct
- **Fine-tuning with LoRA/PEFT** for efficient training
- **Code-to-documentation** generation with various programming languages
- **Multiple dataset integration**: MBPP, DS-1000, custom datasets

### ğŸ“ Text Enhancement & Paraphrasing
- **T5-based paraphrasing** models for text enhancement
- **Style transformation** (imperative to declarative, etc.)
- **Multiple paraphrasing strategies** with different model architectures
- **Batch processing** capabilities for large datasets

## ğŸ—‚ï¸ Project Structure

```
APIDocumentation_Workbench/
â”œâ”€â”€ config/                  # YAML and pipeline configs
â”œâ”€â”€ data/                    # Raw and processed datasets
â”œâ”€â”€ datasets/                # External and custom datasets
â”œâ”€â”€ models/                  # Model checkpoints and cards
â”œâ”€â”€ results/                 # Output results
â”œâ”€â”€ evaluation_results/      # Evaluation metrics and tables
â”œâ”€â”€ logs/                    # Training and experiment logs
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ training/            # Training scripts
â”‚   â”œâ”€â”€ evaluation/          # Evaluation scripts
â”‚   â”œâ”€â”€ util/                # Utilities and helpers
â”‚   â”œâ”€â”€ dataset/             # Dataset extraction/fixing
â”‚   â”œâ”€â”€ test/                # Model testing scripts
â”‚   â”œâ”€â”€ inference/           # Inference utilities
â”‚   â””â”€â”€ snippets/            # Miscellaneous code snippets
â”œâ”€â”€ full_training_pipeline.py # Main pipeline orchestrator
â”œâ”€â”€ quick_start_pipeline.py   # Quick testing pipeline
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ python-env.yml            # Conda environment file
â””â”€â”€ ...
```

## ğŸ”§ Configuration

- All config files are in `config/`
- Document the purpose of each config file in the README
- Example: `axolotl_config.yaml` for Axolotl training

## ğŸ¯ Usage Examples

### Website Documentation Q&A

```python
from scripts.full_training_pipeline import WebCrawler, DataProcessor, ModelTrainer, QASystem
# ...existing code...
```

### Code Documentation Generation

```python
from scripts.training.training import train_documentation_model
# ...existing code...
```

### Text Paraphrasing & Enhancement

```python
from transformers import pipeline
# ...existing code...
```

## ğŸ“Š Datasets

**Available Datasets:**
- **DS-1000**: 1000 data science programming problems (`datasets/ds-1000/`)
- **MBPP**: Mostly Basic Python Programming problems (`datasets/mbpp/`)
- **TAPACO**: Paraphrasing dataset with 73,000+ pairs (`datasets/tapaco/`)
- **Custom Documentation**: Extracted from various API documentation sites (`datasets/website/`)
- **Synthetic & Real Datasets**: Located in `data/` and `datasets/`

### Dataset Format
All datasets follow a consistent JSON structure:
```json
{
    "code": "def example_function(x): return x * 2",
    "documentation": "This function multiplies the input by 2 and returns the result"
}
```

## ğŸ”¬ Experiments & Testing

The repository includes several experimental scripts:

- `rephrasing_test.py` - Basic paraphrasing experiments
- `rephrasing_test2-5.py` - Advanced paraphrasing with different models
- `prompt_*_test.py` - Prompt engineering experiments
- `workbench*.py` - Development and debugging utilities

## ğŸ‹ï¸ Training Details

## ğŸ‹ï¸ Training & Evaluation

- Train models with scripts in `scripts/training/`
- Evaluate with `scripts/evaluation/model_evaluation.py`
- Results saved in `evaluation_results/`

## ğŸ§  Models

- Model checkpoints in `models/`
- Model cards and tokenizer files in each model directory
- Supported models: SmolLM2, Qwen2.5-Coder, T5 variants, and more

## ğŸ›ï¸ Advanced Configuration

### Custom Model Training

```python
# training/api_documentation/training.py
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]
)
```

### Generation Parameters

```python
generation_config = {
    "max_new_tokens": 150,
    "temperature": 0.7,
    "top_p": 0.9,
    "do_sample": True,
    "repetition_penalty": 1.1
}
```

## ğŸ“ˆ Performance & Benchmarks

- Training Speed: ~2-3 hours on RTX 4090 for full training
- Memory Usage: 6-8GB GPU memory for inference
- Generation Quality: BLEU scores of 0.65+ on test sets
- Supported Languages: Python, JavaScript, Java, C++, and more

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
    - Reduce batch size in config
2. **Model Loading Errors**
    - Clear transformers cache
3. **Website Crawling Issues**
    - Check robots.txt and add delays

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Hugging Face** for the transformers library and model hosting
- **SmolLM2** and **Qwen2.5-Coder** teams for the base models
- **DS-1000** and **MBPP** dataset creators
- **TAPACO** paraphrasing dataset contributors

## ğŸ“š Citation

If you use this work in your research, please cite:

```bibtex
@repository{api-documentation-pipeline,
  title={API Documentation Training Pipeline},
  author={Syed Abdul Rahim},
  year={2025},
  url={https://github.com/sarpsl/APIDocumentation}
}
```

---

For questions and support, please open an issue or reach out via the discussions tab.
