# API Documentation Training Pipeline

> **A comprehensive machine learning pipeline for generating and improving API documentation using state-of-the-art language models.**

This repository contains a complete pipeline for automated API documentation generation, paraphrasing, and enhancement using various pre-trained and fine-tuned language models including SmolLM2, Qwen2.5-Coder, and T5.

## ğŸ¯ Project Overview

The project consists of three main components:

1. **ğŸ“– Website Content Extraction & Q&A Generation** - Crawl documentation websites and create Q&A training data
2. **ğŸ§  Code Documentation Generation** - Train models to generate documentation from source code
3. **ğŸ”„ Text Paraphrasing & Enhancement** - Improve existing documentation through rewriting and style transformation

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- CUDA-compatible GPU (recommended)
- 8GB+ RAM

### Installation

```bash
# Clone the repository
git clone https://github.com/sarpsl/APIDocumentation.git
cd APIDocumentation

# Install dependencies
pip install -r requirements_pipeline.txt
```

### Quick Test

```bash
# Run a quick test with minimal training
python quick_start_pipeline.py
```

### Full Pipeline

```bash
# Run the complete pipeline
python full_training_pipeline.py
```

## ğŸ“‹ Features & Capabilities

### ğŸŒ Website Content Processing
- **Recursive web crawling** with configurable depth and filtering
- **Content cleaning** and structured extraction
- **Automatic Q&A pair generation** from documentation content
- **Smart filtering** to exclude non-relevant files (PDFs, images, etc.)

### ğŸ’» Code Documentation Generation
- **Multi-model support**: SmolLM2-360M-Instruct, Qwen2.5-Coder-0.5B-Instruct
- **Fine-tuning with LoRA/PEFT** for efficient training
- **Code-to-documentation** generation with various programming languages
- **Multiple dataset integration**: MBPP, DS-1000, custom datasets

### ğŸ“ Text Enhancement & Paraphrasing
- **T5-based paraphrasing** models for text enhancement
- **Style transformation** (imperative to declarative, etc.)
- **Multiple paraphrasing strategies** with different model architectures
- **Batch processing** capabilities for large datasets

## ğŸ—‚ï¸ Project Structure

```
APIDocumentation/
â”œâ”€â”€ ğŸ“ datasets/                      # Training and evaluation datasets
â”‚   â”œâ”€â”€ ds-1000/                     # DS-1000 dataset for data science
â”‚   â”œâ”€â”€ mbpp/                        # Mostly Basic Python Programming dataset  
â”‚   â”œâ”€â”€ final/                       # Processed final training datasets
â”‚   â”œâ”€â”€ tapaco/                      # Paraphrasing dataset
â”‚   â””â”€â”€ website/                     # Crawled website content
â”œâ”€â”€ ğŸ“ training/                      # Training scripts and models
â”‚   â”œâ”€â”€ api_documentation/           # Code documentation training
â”‚   â””â”€â”€ website_crawler/             # Web crawling and Q&A training
â”œâ”€â”€ ğŸ“„ full_training_pipeline.py     # Main pipeline orchestrator
â”œâ”€â”€ ğŸ“„ quick_start_pipeline.py       # Quick testing pipeline
â”œâ”€â”€ ğŸ“„ pipeline_config.json          # Configuration file
â”œâ”€â”€ ğŸ“„ requirements_pipeline.txt     # Python dependencies
â”œâ”€â”€ ğŸ“„ train_t5_model_paraphrase.py  # T5 paraphrasing model training
â”œâ”€â”€ ğŸ“„ rephrasing_test*.py           # Paraphrasing experiments
â””â”€â”€ ğŸ“„ script_fix_dataset*.py        # Dataset processing utilities
```

## ğŸ”§ Configuration

### Pipeline Configuration (`pipeline_config.json`)

```json
{
    "crawling": {
        "start_url": "https://your-documentation-site.com",
        "max_pages": 50,
        "delay_seconds": 1
    },
    "training": {
        "model_name": "HuggingFaceTB/SmolLM2-360M-Instruct",
        "num_train_epochs": 3,
        "per_device_train_batch_size": 2,
        "max_length": 512
    },
    "generation": {
        "max_new_tokens": 150,
        "temperature": 0.7,
        "top_p": 0.9
    }
}
```

### Model Selection

The project supports multiple base models:

- **SmolLM2-360M-Instruct** - Lightweight, fast inference
- **Qwen2.5-Coder-0.5B-Instruct** - Specialized for code understanding
- **T5 variants** - For paraphrasing and text transformation

## ğŸ¯ Usage Examples

### 1. Website Documentation Q&A

```python
from full_training_pipeline import WebCrawler, DataProcessor, ModelTrainer, QASystem

# Crawl documentation website
crawler = WebCrawler(max_pages=20)
pages = crawler.crawl_website("https://your-docs.com")

# Generate Q&A pairs
processor = DataProcessor()
qa_pairs = processor.generate_qa_pairs(pages)

# Train model
trainer = ModelTrainer()
dataset = trainer.prepare_training_data(qa_pairs)
model_path = trainer.train_model(dataset, "./custom_qa_model")

# Use the trained model
qa_system = QASystem(model_path)
answer = qa_system.ask_question("How do I configure the API?")
```

### 2. Code Documentation Generation

```python
from training.api_documentation.training import train_documentation_model

# Train on your code dataset
model_path = train_documentation_model(
    dataset_path="./datasets/final/final_dataset.json",
    model_name="Qwen/Qwen2.5-Coder-0.5B-Instruct",
    output_dir="./my_code_docs_model"
)

# Generate documentation
code = "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)"
documentation = generate_docs(code, model_path)
```

### 3. Text Paraphrasing & Enhancement

```python
from transformers import pipeline

# Load paraphrasing pipeline
paraphraser = pipeline("text2text-generation", model="Vamsi/T5_Paraphrase_Paws")

# Paraphrase documentation
original = "Write a function that takes a string and returns it in uppercase."
paraphrased = paraphraser(f"Paraphrase: {original}", max_length=64)
```

## ğŸ“Š Datasets

### Built-in Datasets

- **DS-1000**: 1000 data science programming problems
- **MBPP**: Mostly Basic Python Programming problems
- **TAPACO**: Paraphrasing dataset with 73,000+ pairs
- **Custom Documentation**: Extracted from various API documentation sites

### Dataset Formats

All datasets follow a consistent JSON structure:
```json
{
  "code": "def example_function(x): return x * 2",
  "documentation": "This function multiplies the input by 2 and returns the result"
}
```

## ğŸ”¬ Experiments & Testing

The repository includes several experimental scripts:

- `rephrasing_test.py` - Basic paraphrasing experiments
- `rephrasing_test2-5.py` - Advanced paraphrasing with different models
- `prompt_*_test.py` - Prompt engineering experiments
- `workbench*.py` - Development and debugging utilities

## ğŸ‹ï¸ Training Details

### Code Documentation Models
- **Architecture**: Transformer-based language models with LoRA adapters
- **Training**: Instruction tuning format with supervised fine-tuning
- **Optimization**: AdamW with cosine learning rate scheduling
- **Evaluation**: BLEU and METEOR metrics for generation quality

### Paraphrasing Models
- **Base Models**: T5-small, T5-base, T5-large variants
- **Task**: Sequence-to-sequence text transformation
- **Specialized**: Imperative-to-declarative style conversion

## ğŸ›ï¸ Advanced Configuration

### Custom Model Training

```python
# training/api_documentation/training.py
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]
)
```

### Generation Parameters

```python
generation_config = {
    "max_new_tokens": 150,
    "temperature": 0.7,
    "top_p": 0.9,
    "do_sample": True,
    "repetition_penalty": 1.1
}
```

## ğŸ“ˆ Performance & Benchmarks

- **Training Speed**: ~2-3 hours on RTX 4090 for full training
- **Memory Usage**: 6-8GB GPU memory for inference
- **Generation Quality**: BLEU scores of 0.65+ on test sets
- **Supported Languages**: Python, JavaScript, Java, C++, and more

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   ```bash
   # Reduce batch size in config
   "per_device_train_batch_size": 1
   ```

2. **Model Loading Errors**
   ```bash
   # Clear transformers cache
   rm -rf ~/.cache/huggingface/
   ```

3. **Website Crawling Issues**
   ```python
   # Check robots.txt and add delays
   "delay_seconds": 2
   ```

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Hugging Face** for the transformers library and model hosting
- **SmolLM2** and **Qwen2.5-Coder** teams for the base models
- **DS-1000** and **MBPP** dataset creators
- **TAPACO** paraphrasing dataset contributors

## ğŸ“š Citation

If you use this work in your research, please cite:

```bibtex
@repository{api-documentation-pipeline,
  title={API Documentation Training Pipeline},
  author={Syed Abdul Rahim},
  year={2025},
  url={https://github.com/sarpsl/APIDocumentation}
}
```

---

**â­ If this project helps you, please star it on GitHub!**

For questions and support, please open an issue or reach out via the discussions tab.
