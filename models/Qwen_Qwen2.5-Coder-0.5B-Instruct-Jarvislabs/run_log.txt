(venv) root@c10771a35f4d:~/Projects/Qwen-FinalFineTuning# python scripts/final-qwen-0.5B-instruct-ignoreprompt.py 
Map: 100%|███████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 16052.69 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 13759.85 examples/s]
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:16<00:00, 607.09 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 397.31 examples/s]
============================================================
DATASET INFORMATION
============================================================
Training dataset size: 10000 records
Validation dataset size: 1000 records

SAMPLE TRAINING RECORDS:
----------------------------------------
Training Record 1:
  Code: def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):  ...
  Documentation: Trains a k-nearest neighbors classifier for face recognition. :param train_dir: directory that conta...

Training Record 2:
  Code: def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):  if not os.path.isfi...
  Documentation: Recognizes faces in given image using a trained KNN classifier :param X_img_path: path to image to b...

Training Record 3:
  Code: def show_prediction_labels_on_image(img_path, predictions):  pil_image = Image.open(img_path).conver...
  Documentation: Shows the face recognition results visually. :param img_path: path to image to be recognized :param ...

SAMPLE VALIDATION RECORDS:
----------------------------------------
Validation Record 1:
  Code: def nature_cnn(unscaled_images, **conv_kwargs): """ CNN from Nature paper. """ scaled_images = tf.ca...
  Documentation: CNN from Nature paper.

Validation Record 2:
  Code: def mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False): """ Stack of fully-conne...
  Documentation: Stack of fully-connected layers to be used in a policy / q-function approximator Parameters: -------...

Validation Record 3:
  Code: def lstm(nlstm=128, layer_norm=False): """ Builds LSTM (Long-Short Term Memory) network to be used i...
  Documentation: Builds LSTM (Long-Short Term Memory) network to be used in a policy. Note that the resulting functio...

TOKENIZATION STATISTICS:
----------------------------------------
Training - Average text length: 797.5 characters
Training - Min/Max text length: 305/2155 characters
Validation - Average text length: 1015.2 characters
Validation - Min/Max text length: 203/2340 characters

Starting training in 3 seconds...
============================================================
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
No checkpoint found, starting fresh training
  0%|                                                                                                               | 0/5000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 2.0274, 'grad_norm': 1.6112571954727173, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.02}                                     
{'loss': 1.8187, 'grad_norm': 1.1920268535614014, 'learning_rate': 9.8e-05, 'epoch': 0.04}                                                   
{'loss': 1.7112, 'grad_norm': 1.164290428161621, 'learning_rate': 9.7e-05, 'epoch': 0.06}                                                    
{'loss': 1.6671, 'grad_norm': 1.7847542762756348, 'learning_rate': 9.6e-05, 'epoch': 0.08}                                                   
{'loss': 1.6561, 'grad_norm': 2.133531093597412, 'learning_rate': 9.5e-05, 'epoch': 0.1}                                                     
{'loss': 1.7052, 'grad_norm': 1.6724259853363037, 'learning_rate': 9.4e-05, 'epoch': 0.12}                                                   
{'loss': 1.6809, 'grad_norm': 1.258670449256897, 'learning_rate': 9.300000000000001e-05, 'epoch': 0.14}                                      
{'loss': 1.6653, 'grad_norm': 2.3510184288024902, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.16}                                     
{'loss': 1.6508, 'grad_norm': 1.264411211013794, 'learning_rate': 9.1e-05, 'epoch': 0.18}                                                    
{'loss': 1.6094, 'grad_norm': 1.489363193511963, 'learning_rate': 9e-05, 'epoch': 0.2}                                                       
{'loss': 1.6242, 'grad_norm': 1.183869481086731, 'learning_rate': 8.900000000000001e-05, 'epoch': 0.22}                                      
{'loss': 1.6338, 'grad_norm': 1.4585314989089966, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.24}                                     
{'loss': 1.6736, 'grad_norm': 1.3670072555541992, 'learning_rate': 8.7e-05, 'epoch': 0.26}                                                   
{'loss': 1.6507, 'grad_norm': 1.1588914394378662, 'learning_rate': 8.6e-05, 'epoch': 0.28}                                                   
{'loss': 1.6074, 'grad_norm': 1.4044909477233887, 'learning_rate': 8.5e-05, 'epoch': 0.3}                                                    
{'loss': 1.6224, 'grad_norm': 1.3853299617767334, 'learning_rate': 8.4e-05, 'epoch': 0.32}                                                   
{'loss': 1.691, 'grad_norm': 1.4918780326843262, 'learning_rate': 8.3e-05, 'epoch': 0.34}                                                    
{'loss': 1.6139, 'grad_norm': 1.6215797662734985, 'learning_rate': 8.2e-05, 'epoch': 0.36}                                                   
{'loss': 1.6043, 'grad_norm': 1.288291335105896, 'learning_rate': 8.102000000000001e-05, 'epoch': 0.38}                                      
{'loss': 1.6546, 'grad_norm': 1.5492266416549683, 'learning_rate': 8.002000000000001e-05, 'epoch': 0.4}                                      
{'eval_loss': 1.461740493774414, 'eval_runtime': 47.6533, 'eval_samples_per_second': 20.985, 'eval_steps_per_second': 5.246, 'epoch': 0.4}   
 20%|████████████████████                                                                                | 1000/5000 [09:53<36:06,  1.85it/s/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 1.6483, 'grad_norm': 1.5140408277511597, 'learning_rate': 7.902e-05, 'epoch': 0.42}                                                 
{'loss': 1.6477, 'grad_norm': 1.5014197826385498, 'learning_rate': 7.802e-05, 'epoch': 0.44}                                                 
{'loss': 1.6199, 'grad_norm': 1.2928143739700317, 'learning_rate': 7.702e-05, 'epoch': 0.46}                                                 
{'loss': 1.5631, 'grad_norm': 1.6965694427490234, 'learning_rate': 7.602000000000001e-05, 'epoch': 0.48}                                     
{'loss': 1.5867, 'grad_norm': 1.2109721899032593, 'learning_rate': 7.502e-05, 'epoch': 0.5}                                                  
{'loss': 1.6047, 'grad_norm': 1.7186102867126465, 'learning_rate': 7.402e-05, 'epoch': 0.52}                                                 
{'loss': 1.7133, 'grad_norm': 2.028991937637329, 'learning_rate': 7.302e-05, 'epoch': 0.54}                                                  
{'loss': 1.5669, 'grad_norm': 1.0113414525985718, 'learning_rate': 7.202e-05, 'epoch': 0.56}                                                 
{'loss': 1.6146, 'grad_norm': 1.6496814489364624, 'learning_rate': 7.102000000000001e-05, 'epoch': 0.58}                                     
{'loss': 1.6224, 'grad_norm': 1.329323172569275, 'learning_rate': 7.002000000000001e-05, 'epoch': 0.6}                                       
{'loss': 1.5887, 'grad_norm': 1.4677088260650635, 'learning_rate': 6.902000000000001e-05, 'epoch': 0.62}                                     
{'loss': 1.6494, 'grad_norm': 2.0260729789733887, 'learning_rate': 6.802e-05, 'epoch': 0.64}                                                 
{'loss': 1.6351, 'grad_norm': 1.419406771659851, 'learning_rate': 6.702e-05, 'epoch': 0.66}                                                  
{'loss': 1.6413, 'grad_norm': 1.2132362127304077, 'learning_rate': 6.602000000000001e-05, 'epoch': 0.68}                                     
{'loss': 1.6203, 'grad_norm': 1.0488771200180054, 'learning_rate': 6.502e-05, 'epoch': 0.7}                                                  
{'loss': 1.5615, 'grad_norm': 1.3922901153564453, 'learning_rate': 6.402e-05, 'epoch': 0.72}                                                 
{'loss': 1.6551, 'grad_norm': 1.6099292039871216, 'learning_rate': 6.302e-05, 'epoch': 0.74}                                                 
{'loss': 1.5894, 'grad_norm': 1.4687912464141846, 'learning_rate': 6.202e-05, 'epoch': 0.76}                                                 
{'loss': 1.6016, 'grad_norm': 1.5931782722473145, 'learning_rate': 6.102e-05, 'epoch': 0.78}                                                 
{'loss': 1.6232, 'grad_norm': 1.736912488937378, 'learning_rate': 6.002e-05, 'epoch': 0.8}                                                   
{'eval_loss': 1.4497874975204468, 'eval_runtime': 47.7982, 'eval_samples_per_second': 20.921, 'eval_steps_per_second': 5.23, 'epoch': 0.8}   
 40%|████████████████████████████████████████                                                            | 2000/5000 [19:45<27:14,  1.84it/s/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 1.5764, 'grad_norm': 1.3911793231964111, 'learning_rate': 5.902e-05, 'epoch': 0.82}                                                 
{'loss': 1.63, 'grad_norm': 1.3501031398773193, 'learning_rate': 5.802000000000001e-05, 'epoch': 0.84}                                       
{'loss': 1.6071, 'grad_norm': 1.533913016319275, 'learning_rate': 5.7020000000000006e-05, 'epoch': 0.86}                                     
{'loss': 1.5864, 'grad_norm': 1.3948954343795776, 'learning_rate': 5.602000000000001e-05, 'epoch': 0.88}                                     
{'loss': 1.6043, 'grad_norm': 1.0988022089004517, 'learning_rate': 5.5020000000000005e-05, 'epoch': 0.9}                                     
{'loss': 1.6106, 'grad_norm': 1.5916239023208618, 'learning_rate': 5.402e-05, 'epoch': 0.92}                                                 
{'loss': 1.6414, 'grad_norm': 1.7874972820281982, 'learning_rate': 5.3020000000000004e-05, 'epoch': 0.94}                                    
{'loss': 1.4996, 'grad_norm': 1.486932396888733, 'learning_rate': 5.202e-05, 'epoch': 0.96}                                                  
{'loss': 1.5903, 'grad_norm': 1.9566011428833008, 'learning_rate': 5.102e-05, 'epoch': 0.98}                                                 
{'loss': 1.6509, 'grad_norm': 0.9063317775726318, 'learning_rate': 5.002e-05, 'epoch': 1.0}                                                  
{'loss': 1.6372, 'grad_norm': 1.4253129959106445, 'learning_rate': 4.902e-05, 'epoch': 1.02}                                                 
{'loss': 1.6015, 'grad_norm': 1.396167278289795, 'learning_rate': 4.8020000000000004e-05, 'epoch': 1.04}                                     
{'loss': 1.5878, 'grad_norm': 1.962284803390503, 'learning_rate': 4.702e-05, 'epoch': 1.06}                                                  
{'loss': 1.5477, 'grad_norm': 1.0743408203125, 'learning_rate': 4.602e-05, 'epoch': 1.08}                                                    
{'loss': 1.5771, 'grad_norm': 1.690209984779358, 'learning_rate': 4.502e-05, 'epoch': 1.1}                                                   
{'loss': 1.5777, 'grad_norm': 1.5667953491210938, 'learning_rate': 4.402e-05, 'epoch': 1.12}                                                 
{'loss': 1.6066, 'grad_norm': 1.180351972579956, 'learning_rate': 4.3020000000000005e-05, 'epoch': 1.14}                                     
{'loss': 1.5029, 'grad_norm': 1.4454855918884277, 'learning_rate': 4.202e-05, 'epoch': 1.16}                                                 
{'loss': 1.5994, 'grad_norm': 1.3825621604919434, 'learning_rate': 4.1020000000000004e-05, 'epoch': 1.18}                                    
{'loss': 1.5929, 'grad_norm': 1.446458339691162, 'learning_rate': 4.002e-05, 'epoch': 1.2}                                                   
{'eval_loss': 1.447583794593811, 'eval_runtime': 46.9183, 'eval_samples_per_second': 21.314, 'eval_steps_per_second': 5.328, 'epoch': 1.2}   
 60%|████████████████████████████████████████████████████████████                                        | 3000/5000 [29:33<17:56,  1.86it/s/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 1.579, 'grad_norm': 1.6272788047790527, 'learning_rate': 3.902e-05, 'epoch': 1.22}                                                  
{'loss': 1.5667, 'grad_norm': 1.6806354522705078, 'learning_rate': 3.802e-05, 'epoch': 1.24}                                                 
{'loss': 1.6263, 'grad_norm': 1.5621408224105835, 'learning_rate': 3.702e-05, 'epoch': 1.26}                                                 
{'loss': 1.5424, 'grad_norm': 1.1551979780197144, 'learning_rate': 3.6020000000000004e-05, 'epoch': 1.28}                                    
{'loss': 1.6409, 'grad_norm': 1.4330376386642456, 'learning_rate': 3.502e-05, 'epoch': 1.3}                                                  
{'loss': 1.5727, 'grad_norm': 1.4421207904815674, 'learning_rate': 3.402e-05, 'epoch': 1.32}                                                 
{'loss': 1.5892, 'grad_norm': 1.660967230796814, 'learning_rate': 3.302e-05, 'epoch': 1.34}                                                  
{'loss': 1.6331, 'grad_norm': 1.1870604753494263, 'learning_rate': 3.202e-05, 'epoch': 1.36}                                                 
{'loss': 1.5274, 'grad_norm': 1.435247540473938, 'learning_rate': 3.102e-05, 'epoch': 1.38}                                                  
{'loss': 1.6225, 'grad_norm': 1.2975618839263916, 'learning_rate': 3.0020000000000004e-05, 'epoch': 1.4}                                     
{'loss': 1.5928, 'grad_norm': 1.4129517078399658, 'learning_rate': 2.9020000000000003e-05, 'epoch': 1.42}                                    
{'loss': 1.5476, 'grad_norm': 1.1060911417007446, 'learning_rate': 2.8020000000000003e-05, 'epoch': 1.44}                                    
{'loss': 1.6268, 'grad_norm': 1.0914579629898071, 'learning_rate': 2.7020000000000002e-05, 'epoch': 1.46}                                    
{'loss': 1.5501, 'grad_norm': 1.4618232250213623, 'learning_rate': 2.602e-05, 'epoch': 1.48}                                                 
{'loss': 1.6213, 'grad_norm': 1.4407697916030884, 'learning_rate': 2.5019999999999998e-05, 'epoch': 1.5}                                     
{'loss': 1.6051, 'grad_norm': 1.36958909034729, 'learning_rate': 2.402e-05, 'epoch': 1.52}                                                   
{'loss': 1.5199, 'grad_norm': 1.762847661972046, 'learning_rate': 2.302e-05, 'epoch': 1.54}                                                  
{'loss': 1.5839, 'grad_norm': 1.4289993047714233, 'learning_rate': 2.2020000000000003e-05, 'epoch': 1.56}                                    
{'loss': 1.5713, 'grad_norm': 1.3943535089492798, 'learning_rate': 2.1020000000000002e-05, 'epoch': 1.58}                                    
{'loss': 1.5785, 'grad_norm': 1.326324462890625, 'learning_rate': 2.002e-05, 'epoch': 1.6}                                                   
{'eval_loss': 1.4513607025146484, 'eval_runtime': 47.1248, 'eval_samples_per_second': 21.22, 'eval_steps_per_second': 5.305, 'epoch': 1.6}   
 80%|████████████████████████████████████████████████████████████████████████████████                    | 4000/5000 [39:20<08:56,  1.86it/s/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/Projects/Qwen-FinalFineTuning/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 1.5571, 'grad_norm': 1.4269700050354004, 'learning_rate': 1.902e-05, 'epoch': 1.62}                                                 
{'loss': 1.5648, 'grad_norm': 1.3061703443527222, 'learning_rate': 1.802e-05, 'epoch': 1.64}                                                 
{'loss': 1.5937, 'grad_norm': 1.094533920288086, 'learning_rate': 1.702e-05, 'epoch': 1.66}                                                  
{'loss': 1.5887, 'grad_norm': 1.1770833730697632, 'learning_rate': 1.6020000000000002e-05, 'epoch': 1.68}                                    
{'loss': 1.5474, 'grad_norm': 1.2569630146026611, 'learning_rate': 1.502e-05, 'epoch': 1.7}                                                  
{'loss': 1.5341, 'grad_norm': 1.5275254249572754, 'learning_rate': 1.402e-05, 'epoch': 1.72}                                                 
{'loss': 1.5925, 'grad_norm': 1.5823948383331299, 'learning_rate': 1.3020000000000002e-05, 'epoch': 1.74}                                    
{'loss': 1.5237, 'grad_norm': 1.3990612030029297, 'learning_rate': 1.202e-05, 'epoch': 1.76}                                                 
{'loss': 1.5573, 'grad_norm': 1.6137655973434448, 'learning_rate': 1.1020000000000001e-05, 'epoch': 1.78}                                    
{'loss': 1.5568, 'grad_norm': 1.743378758430481, 'learning_rate': 1.002e-05, 'epoch': 1.8}                                                   
{'loss': 1.6138, 'grad_norm': 2.0718421936035156, 'learning_rate': 9.02e-06, 'epoch': 1.82}                                                  
{'loss': 1.5465, 'grad_norm': 1.2566618919372559, 'learning_rate': 8.02e-06, 'epoch': 1.84}                                                  
{'loss': 1.5629, 'grad_norm': 1.7137237787246704, 'learning_rate': 7.0200000000000006e-06, 'epoch': 1.86}                                    
{'loss': 1.583, 'grad_norm': 1.4788578748703003, 'learning_rate': 6.040000000000001e-06, 'epoch': 1.88}                                      
{'loss': 1.6161, 'grad_norm': 1.3116809129714966, 'learning_rate': 5.04e-06, 'epoch': 1.9}                                                   
{'loss': 1.5368, 'grad_norm': 1.5336705446243286, 'learning_rate': 4.04e-06, 'epoch': 1.92}                                                  
{'loss': 1.6497, 'grad_norm': 1.401573657989502, 'learning_rate': 3.04e-06, 'epoch': 1.94}                                                   
{'loss': 1.5769, 'grad_norm': 1.0894489288330078, 'learning_rate': 2.0400000000000004e-06, 'epoch': 1.96}                                    
{'loss': 1.5782, 'grad_norm': 2.068631887435913, 'learning_rate': 1.04e-06, 'epoch': 1.98}                                                   
{'loss': 1.5941, 'grad_norm': 1.3134475946426392, 'learning_rate': 4e-08, 'epoch': 2.0}                                                      
{'eval_loss': 1.450634479522705, 'eval_runtime': 48.1693, 'eval_samples_per_second': 20.76, 'eval_steps_per_second': 5.19, 'epoch': 2.0}     
{'train_runtime': 2957.8742, 'train_samples_per_second': 6.762, 'train_steps_per_second': 1.69, 'train_loss': 1.6092056228637694, 'epoch': 2.0}                                                                                                                                           
100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [49:17<00:00,  1.69it/s]